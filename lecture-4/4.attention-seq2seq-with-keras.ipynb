{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-based Encoder–Decoder  with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives for this Notebook    \n",
    "* Build an LSTM-based Encoder–Decoder (Seq2Seq) model with attention using the Keras library\n",
    "*  Understand how encoder–decoder attention (Bahdanau/Additive Attention) improves sequence-to-sequence learning\n",
    "* Train an attention-based Seq2Seq model on a given dataset using teacher forcing\n",
    "* Use the trained model to perform sequence-to-sequence translation\n",
    "* Analyze model performance by visualizing training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 4>\n",
    "1. <a href=\"#Import-Keras-and-Packages\">Import Keras and Packages</a><br>\n",
    "2. <a href=\"#Step-1:-Data-Preparation\">Step 1: Data Preparation</a><br>\n",
    "3. <a href=\"#Step-2:-Self-Attention-Layer\">Step 2: Self-Attention Layer</a><br>\n",
    "4. <a href=\"#Step-3:-Model-Architecture\">Step 3: Model Architecture</a><br>\n",
    "5. <a href=\"#Step-4:-Training-the-Model\">Step 4: Training the Model</a><br>\n",
    "6. <a href=\"#Step-5:-Plotting-the-training-loss\">Step 5: Plotting the training loss</a><br>\n",
    "\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the keras libraries and the packages that we would need to build a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_cpu==2.17.1 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (2.17.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (0.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (3.15.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (4.25.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (1.76.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (3.13.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorflow_cpu==2.17.1) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow_cpu==2.17.1) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow_cpu==2.17.1) (14.2.0)\n",
      "Requirement already satisfied: namex in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow_cpu==2.17.1) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow_cpu==2.17.1) (0.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow_cpu==2.17.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow_cpu==2.17.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow_cpu==2.17.1) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow_cpu==2.17.1) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow_cpu==2.17.1) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow_cpu==2.17.1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow_cpu==2.17.1) (3.1.4)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow_cpu==2.17.1) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow_cpu==2.17.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow_cpu==2.17.1) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow_cpu==2.17.1) (0.1.2)\n",
      "Requirement already satisfied: matplotlib==3.9.2 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from matplotlib==3.9.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from matplotlib==3.9.2) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from matplotlib==3.9.2) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from matplotlib==3.9.2) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from matplotlib==3.9.2) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from matplotlib==3.9.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/aq/installations/python_envs/ml_env/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.17.0)\n",
      "==== All required libraries are installed =====\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_cpu==2.17.1\n",
    "!pip install matplotlib==3.9.2\n",
    "\n",
    "print(\"==== All required libraries are installed =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppress the tensorflow warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To use Keras, you will also need to install a backend framework – such as TensorFlow.\n",
    "You may install the GPU version of tensorflow on your machine to accelarate the processing of larger datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.layers import Layer\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preparation\n",
    "We start by define the sentences and text for translation training\n",
    "Sentence Pairs: Defines a small dataset of English-Spanish sentence pairs.\n",
    "Target Sequences:\n",
    "Prepends \"startseq\" and appends \"endseq\" to each target sentence for the decoder to learn when to start and stop translating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample parallel sentences (English -> Spanish)\n",
    "input_texts = [\n",
    "    \"Hello.\", \"How are you?\"\n",
    "]\n",
    "target_texts = [\n",
    "    \"Hola.\", \"¿Cómo estás?\"\n",
    "]\n",
    "\n",
    "target_texts = [\"startseq \" + x + \" endseq\" for x in target_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we convert the text from the sentences to tokens and create a vocabulary\n",
    "Tokenization: Uses Tokenizer to convert words into numerical sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 5\n",
      "output_vocab_size: 6\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "\n",
    "output_tokenizer = Tokenizer()\n",
    "output_tokenizer.fit_on_texts(target_texts)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1\n",
    "print(f\"input_vocab_size: {input_vocab_size}\")\n",
    "print(f\"output_vocab_size: {output_vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now pad the corresponding sentences\n",
    "Padding: Ensures all sequences have the same length while masking ignores padded tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_input_length: 3\n",
      "max_output_length: 4\n",
      "input_sequences: [[1 0 0]\n",
      " [2 3 4]]\n",
      "output_sequences: [[1 3 2 0]\n",
      " [1 4 5 2]]\n"
     ]
    }
   ],
   "source": [
    "# Padding\n",
    "max_input_length = max([len(seq) for seq in input_sequences])\n",
    "max_output_length = max([len(seq) for seq in output_sequences])\n",
    "print(f\"max_input_length: {max_input_length}\")\n",
    "print(f\"max_output_length: {max_output_length}\")\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=max_output_length, padding='post')\n",
    "print(f\"input_sequences: {input_sequences}\")\n",
    "print(f\"output_sequences: {output_sequences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_data: [[1 3 2]\n",
      " [1 4 5]]\n",
      "decoder_output_data: [[3 2 0]\n",
      " [4 5 2]]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the target data for training\n",
    "decoder_input_data = output_sequences[:, :-1]\n",
    "decoder_output_data = output_sequences[:, 1:]\n",
    "print(f\"decoder_input_data: {decoder_input_data}\")\n",
    "print(f\"decoder_output_data: {decoder_output_data}\")\n",
    "# Convert to one-hot\n",
    "decoder_output_data = np.array([np.eye(output_vocab_size)[seq] for seq in decoder_output_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Self-Attention Layer\n",
    "Self-attention is a mechanism that allows a model to **focus on relevant parts of the input sequence** while processing each word. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention class\n",
    "In this implementation of self-attention layer:\n",
    "1. We first initialize the weights in the **build** method, where:\n",
    "    1. **self.Wq**, **self.Wk**, **self.Wv** are the trainable weight matrices.\n",
    "    2. Their **shape is (feature_dim, feature_dim)**, meaning they transform input features into Q, K, and V representations.\n",
    "2. Applying Attention using **call** method. The **call()** method:\n",
    "   1. Computes **Q, K, V** by multiplying inputs (encoder/decoder output) with their respective weight matrices.\n",
    "   2. Computes **dot-product attention scores** using K.batch_dot(q, k, axes=[2, 2]), resulting in a (batch_size, seq_len, seq_len) matrix.\n",
    "   3. **Scales** the scores to avoid large values.\n",
    "   4. Applies **softmax** to normalize the attention scores.\n",
    "   5. **Multiplies attention weights with V** to get the final output.\n",
    "3. The **compute_output_shape** method defines the shape of the output tensor after the layer processes an input.\n",
    "    1. The output shape of the Self-Attention layer **remains the same** as the input shape.\n",
    "    2. The attention mechanism **transforms** the input but does not change its dimensions.4\n",
    "    3. If the attention layer changed the shape, you would modify compute_output_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Self-Attention Layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1] # Feature dimension (embedding size)\n",
    "        # Trainable weight matrix to generate Query vectors\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', #sets how the initial values of the weights are generated\n",
    "                                  trainable=True, # weights updated via backpropagation\n",
    "                                  name='Wq') # gives the weight variable a name\n",
    "        # Trainable weight matrix to generate Key vectors\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "         # Trainable weight matrix to generate Value vectors\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Architecture\n",
    "The model follows an Encoder-Decoder structure:\n",
    "\n",
    "### Encoder:\n",
    "1) Takes input sentences (padded and tokenized).\n",
    "2) Uses an Embedding layer (word representations) + LSTM (to process sequences).\n",
    "    1. The LSTMs are used as the **help process variable-length input sentences** and generate meaningful translations.\n",
    "4) Outputs context vectors (hidden & cell states).\n",
    "\n",
    "### Attention Layer\n",
    "1) Applied to both the encoder and decoder outputs.\n",
    "2) Helps the decoder focus on relevant words during translation.\n",
    "\n",
    "### Decoder\n",
    "1) Receives target sequences (shifted one step ahead).\n",
    "2) Uses an LSTM with encoder states as initial states.\n",
    "3) Applies self-attention for better learning.\n",
    "4) Uses a Dense layer (Softmax) to predict the next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_4         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ additive_attention  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AdditiveAttention</span>) │                   │            │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ additive_attenti… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,078</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_4         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m1,280\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m1,536\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ additive_attention  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m256\u001b[0m │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mAdditiveAttention\u001b[0m) │                   │            │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ additive_attenti… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │      \u001b[38;5;34m3,078\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,056,774</span> (4.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,056,774\u001b[0m (4.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,056,774</span> (4.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,056,774\u001b[0m (4.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention, Concatenate, Dense, Embedding, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    " \n",
    "# Encoder\n",
    "## Encoder input\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "## Embedding layer with 256  LSTM units (also called latent dimension).\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "## Encoder LSTM\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "# encoder_outputs → hidden states for all time steps\n",
    "# state_h → final hidden state of the encoder LSTM\n",
    "# state_c → final cell (memory) state of the encoder LSTM\n",
    "encoder_states = [state_h, state_c]\n",
    " \n",
    "# Decoder input\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,))\n",
    "# Decoder embedding\n",
    "decoder_embedding = Embedding(output_vocab_size, 256)(decoder_inputs)\n",
    "# Decoder LSTM (initialized with encoder states)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    " \n",
    "# Attention: decoder attends to encoder outputs\n",
    "attention = AdditiveAttention()\n",
    "attention_output = attention([decoder_outputs, # Decoder outputs → queries\n",
    "                              encoder_outputs] # Encoder outputs → keys & values\n",
    "                            )\n",
    "# APPLY SELF-ATTENTION HERE\n",
    "#self_attention = SelfAttention(name=\"encoder_self_attention\")\n",
    "#attention_output = self_attention(encoder_outputs)\n",
    "\n",
    "# Combine decoder outputs with attention context\n",
    "decoder_concat = Concatenate(axis=-1)([decoder_outputs, attention_output])\n",
    " \n",
    "# Final Dense layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    " \n",
    "# Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Training the Model\n",
    "Uses categorical_crossentropy as the loss function since output words are one-hot encoded.\n",
    "Trains using Adam optimizer for 100 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.1667 - loss: 1.7919\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 1.7612\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3333 - loss: 1.7299\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3333 - loss: 1.6966\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3333 - loss: 1.6599\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3333 - loss: 1.6187\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3333 - loss: 1.5717\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3333 - loss: 1.5179\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3333 - loss: 1.4565\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3333 - loss: 1.3868\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3333 - loss: 1.3090\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3333 - loss: 1.2239\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 1.1330\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 1.0389\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.9454\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6667 - loss: 0.8583\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6667 - loss: 0.7803\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8333 - loss: 0.7044\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8333 - loss: 0.6244\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8333 - loss: 0.5467\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.4797\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.4161\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.3547\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.3034\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.2563\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.2118\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1760\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.1475\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.1210\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0968\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0770\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0619\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0500\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0401\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0319\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0254\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0207\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0171\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0142\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0118\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0082\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0070\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0061\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0053\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0047\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0042\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0034\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0031\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0028\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 9.9771e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 9.7021e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 9.4457e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 9.2053e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 8.9784e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 8.7643e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 8.5610e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 8.3677e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 8.1846e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 8.0106e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 7.8446e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 7.6872e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 7.5373e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 7.3954e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 7.2600e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 7.1314e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 7.0089e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 6.8921e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 6.7802e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 6.6736e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 6.5711e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 6.4727e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 6.3778e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 6.2862e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 6.1983e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 6.1131e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 6.0307e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 5.9503e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 5.8729e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 5.7974e-04\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Fit the Model\n",
    "history_glorot_adam = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Plotting the training loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['accuracy', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting training loss\n",
    "import matplotlib.pyplot as plt\n",
    "print(history_glorot_adam.history.keys())\n",
    "plt.plot(history_glorot_adam.history['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "prev_pub_hash": "0a2f1c23451e90dafa6342776b11efcb1308959b2f77af45d77f72bbadb754ca"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
